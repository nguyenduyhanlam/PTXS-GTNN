{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Encoder_Decoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOotws0j5Dfqn78QGacTGgZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fTAjC1G4OF1N","colab_type":"text"},"source":["**0) Lý thuyết và nguồn dữ liệu** <br>\n","Nguồn dữ liệu được lấy từ trang web: https://github.com/stefan-it/nmt-en-vi/tree/master/data\n","\n","Lý thuyết tham khảo ở các tài liệu:\n","\n","\n","1.   Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\n","2.   Rico Sennrich, Barry Haddow (2016). Linguistic Input Features Improve Neural Machine Translation.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hxUYaEMU1AaI","colab_type":"text"},"source":["**1) Chuẩn bị thư viện và dữ liệu**"]},{"cell_type":"markdown","metadata":{"id":"4RZDPppFDjtP","colab_type":"text"},"source":["Đầu tiên ta sẽ chuẩn bị một số thư viện"]},{"cell_type":"code","metadata":{"id":"-WgSYmr_0x7R","colab_type":"code","outputId":"cf07fae6-9b64-42bb-ec0f-c0d187fe24ef","executionInfo":{"status":"ok","timestamp":1586850128212,"user_tz":-420,"elapsed":10372,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","!pip install unidecode\n","import unidecode\n","import string\n","import re\n","import random\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 4.7MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IJUJlhLxDqPC","colab_type":"text"},"source":["Chuẩn bị dữ liệu cho chương trình"]},{"cell_type":"code","metadata":{"id":"mUbTp8MXD-qB","colab_type":"code","outputId":"0e540c39-8212-4821-a942-85336faeccf9","executionInfo":{"status":"ok","timestamp":1586850139407,"user_tz":-420,"elapsed":11190,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["!git clone https://github.com/nguyenduyhanlam/PTXS-GTNN\n","!ls\n","import os\n","os.getcwd()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'PTXS-GTNN'...\n","remote: Enumerating objects: 20, done.\u001b[K\n","remote: Counting objects:   5% (1/20)\u001b[K\rremote: Counting objects:  10% (2/20)\u001b[K\rremote: Counting objects:  15% (3/20)\u001b[K\rremote: Counting objects:  20% (4/20)\u001b[K\rremote: Counting objects:  25% (5/20)\u001b[K\rremote: Counting objects:  30% (6/20)\u001b[K\rremote: Counting objects:  35% (7/20)\u001b[K\rremote: Counting objects:  40% (8/20)\u001b[K\rremote: Counting objects:  45% (9/20)\u001b[K\rremote: Counting objects:  50% (10/20)\u001b[K\rremote: Counting objects:  55% (11/20)\u001b[K\rremote: Counting objects:  60% (12/20)\u001b[K\rremote: Counting objects:  65% (13/20)\u001b[K\rremote: Counting objects:  70% (14/20)\u001b[K\rremote: Counting objects:  75% (15/20)\u001b[K\rremote: Counting objects:  80% (16/20)\u001b[K\rremote: Counting objects:  85% (17/20)\u001b[K\rremote: Counting objects:  90% (18/20)\u001b[K\rremote: Counting objects:  95% (19/20)\u001b[K\rremote: Counting objects: 100% (20/20)\u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\n","remote: Compressing objects: 100% (18/18), done.\u001b[K\n","remote: Total 20 (delta 3), reused 19 (delta 2), pack-reused 0\u001b[K\n","Unpacking objects: 100% (20/20), done.\n","PTXS-GTNN  sample_data\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"jyy93Abf3D6C","colab_type":"text"},"source":["**2) Load dữ liệu** <br>\n","Ta sẽ tạo class ngôn ngữ dùng để load dữ liệu từ file text. <br>\n","Class ngôn ngữ gồm có các **thuộc tính** cơ bản sau:\n","*   **name**: tên của ngôn ngữ (English, tiếng Việt,...)\n","*   **word2index**: bộ từ điển.\n","*   **word2count**: bộ đếm từ.\n","*   **index2word**: bộ lưu vị trí các token.\n","*   **n_words**: đếm tổng các lượng từ.\n","\n","Class ngôn ngữ gồm có các **hàm** cơ bản sau:\n","*   **addSentence**: Có tham số là sentence, dùng để truyền vào 1 câu văn bản. Từ câu văn bản này, ta sẽ tách thành các từ (word). Cuối cùng ta sẽ lưu các từ này vào bộ từ điển bằng hàm addWord.\n","*   **addWord**: Có tham số là word, dùng để truyền vào 1 từ (word), nếu từ này chưa có trong bộ từ điển thì sẽ được thêm (add) mới vào bộ từ điển. Nếu đã có rồi ta sẽ tăng số lần xuất hiện (số đếm) của từ này thêm 1 đơn vị.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pM_AUaFqH_zO","colab_type":"code","colab":{}},"source":["SOS_token = 0 #Start Of Sequence\n","EOS_token = 1 #End Of Sequence\n","PAD_token = 2 # Used for padding short sentences\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hOqMx1fwDtaW","colab_type":"text"},"source":["Ta cần triển khai thêm một số hàm bổ trợ như:\n","*   **remove_accent**: dùng để loại bỏ các dấu thanh (sắc, hỏi, huyền, ngã, nặng,...).\n","*   **normalizeString**: Dùng để chuẩn hóa câu để chạy chương trình. Đầu tiên ta sẽ loại bỏ hết các dấu thanh ra khỏi câu. Sau đó ta sẽ tách các dấu câu (dấu phẩy, dấu chấm, dấu chấm hỏi, dấu chấm thang,...) ra khỏi từ.\n","\n","Ví dụ ta có câu: \"Ăn quả, nhớ kẻ trồng cây.\". Sau khi chạy hàm **normalizeString** ta sẽ được kết quả là: \"An qua , nho ke trong cay .\"\n","\n"]},{"cell_type":"code","metadata":{"id":"Qj-oU1nND-F1","colab_type":"code","outputId":"039d8ce7-3c80-49b2-9b85-11ba5db94e99","executionInfo":{"status":"ok","timestamp":1586850146711,"user_tz":-420,"elapsed":1192,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# remove all the accents\n","def remove_accent(utf8_str):\n","    return unidecode.unidecode(utf8_str)\n","\n","# Normalize the string (marks and words are seperated, words don't contain accents,...)\n","def normalizeString(s):\n","    # Remove all the accents first.\n","    s = remove_accent(s)\n","    # Seperate words and marks by adding spaces between them\n","    marks = '[.!?,-${}()]'\n","    r = \"([\"+\"\\\\\".join(marks)+\"])\"\n","    s = re.sub(r, r\" \\1 \", s)\n","    # replace continuous spaces with a single space\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","# Example\n","ex_s = \"Ăn quả, nhớ kẻ trồng cây.\"\n","print(normalizeString(ex_s)) # result will be \"An qua , nho ke trong cay .\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["An qua , nho ke trong cay .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ptWmB5UTttq","colab_type":"text"},"source":["Cuối cùng ta sẽ thực hiện việc load dữ liệu."]},{"cell_type":"code","metadata":{"id":"jpgXPOsdT2vi","colab_type":"code","outputId":"7f4bee1e-45bc-4c15-f98c-d21f9eb9a731","executionInfo":{"status":"ok","timestamp":1586850148880,"user_tz":-420,"elapsed":784,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["def readLangs(lang1, lang2):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines_language1 = open('PTXS-GTNN/tst2013.%s' % lang1, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","    lines_language2 = open('PTXS-GTNN/tst2013.%s' % lang2, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","\n","    # Normalize all the lines\n","    data_language1 = [normalizeString(l) for l in lines_language1]\n","    data_language2 = [normalizeString(l) for l in lines_language2]\n","\n","    # Prepare return values\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","    data = list(zip(data_language1, data_language2))\n","\n","    return input_lang, output_lang, data\n","\n","# Test the function\n","lang1 = \"en\"\n","lang2 = \"vi\"\n","input_lang, output_lang, data = readLangs(lang1, lang2)\n","print(\"Language 1:\", input_lang.name)\n","print(\"Language 2:\", output_lang.name)\n","print(random.choice(data))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading lines...\n","Language 1: en\n","Language 2: vi\n","('In fact , they &apos;re often the initial and convincing negotiators of a bright future for their daughters , but in the context of a society like in Afghanistan , we must have the support of men .', 'That ra , ho la nguoi dau tien va la nguoi thuong thuyet thuyet phuc cho mot tuong lai sang cua con gai ho , nhung trong hoan canh xa hoi nhu Afghanistan , chung ta can den su ung ho cua nhung nguoi dan ong .')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pc_5i5o3hjUK","colab_type":"text"},"source":["**3) Chuẩn bị dữ liệu** <br>\n"]},{"cell_type":"markdown","metadata":{"id":"VPlUxPSnheDR","colab_type":"text"},"source":["Chương trình sẽ chạy nhanh hơn khi chúng ta cắt bộ dữ liệu thành các câu ngắn và đơn giản. Ở đây ta sẽ giới hạn cho 1 câu có tối đa là 10 từ (bao gồm cả dấu chấm câu).\n"]},{"cell_type":"code","metadata":{"id":"oMqJGuwzmfSR","colab_type":"code","colab":{}},"source":["MAX_LENGTH = 10\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayOOFO5war45","colab_type":"text"},"source":["Sau quá trình biến đổi chúng ta sẽ có được tập dữ liệu để chuẩn bị cho huấn luyện."]},{"cell_type":"code","metadata":{"id":"LOuJkWgQhtN4","colab_type":"code","outputId":"e019abe9-eea6-4482-937a-75438a8dceab","executionInfo":{"status":"ok","timestamp":1586850184084,"user_tz":-420,"elapsed":744,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["def prepareData(lang1, lang2):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(\"Language 1:\", input_lang.name, \"There are\", input_lang.n_words, \"different words\")\n","    print(\"Language 2:\", output_lang.name, \"There are\", output_lang.n_words, \"different words\")\n","    return input_lang, output_lang, pairs\n","\n","\n","input_lang, output_lang, pairs = prepareData('en', 'vi')\n","print(random.choice(pairs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading lines...\n","Read 1268 sentence pairs\n","Trimmed to 135 sentence pairs\n","Counting words...\n","Counted words:\n","Language 1: en There are 320 different words\n","Language 2: vi There are 334 different words\n","('Let &apos;s see . Yeah , it worked .', 'Xem ne . Vang , no dinh .')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wR52oDO57ZYc","colab_type":"text"},"source":["**4) Mô hình Encoder-Decoder**\n","\n","Đầu tiên ta sẽ đi xây dưng bộ encoder trước.\n","\n","---\n","\n","===================================**Encoder**===================================\n","\n","Mô hình encoder như sau:\n","\n","![Encoder example](https://drive.google.com/uc?id=1cyor63sz5r_-hnwXvptVDl_4-3ltsKzC)\n","\n","(Source: https://medium.com/@t.schnake/a-formalization-of-a-simple-sequential-encoder-decoder-b31be7e92988)\n","\n","![Pytorch encoder flow](https://drive.google.com/uc?id=1R5dtbS_Xl4wYZMJcL4BT47VEGX-lMMXR)\n","\n","(Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","\n","Ta sẽ đi xây dựng một class có tên là EncoderRNN cho bộ GRU-RNN encoder.\n","\n","Class này có các thuộc tính cơ bản sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong bộ từ điển.\n","*   **embedding**: lưu trữ kết quả của bộ từ điển sau khi thực hiện word embedding (tham khảo thêm về word embedding tại https://machinelearningmastery.com/what-are-word-embeddings/).\n","*   **gru**: cấu trúc GRU sẽ sử dụng.\n","\n","Trong hàm khởi tạo ta sẽ truyền vào các biến sau:\n","*   **input_size**: kích thước của bộ từ điển (số lượng từ có trong bộ từ điển).\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong bộ từ điển.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eL3NOqyGAi87","colab_type":"code","colab":{}},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        # A simple lookup table that stores embeddings of a fixed dictionary \n","        # and size. \n","        # This module is often used to store word embeddings and retrieve them using indices. \n","        # The input to the module is a list of indices, \n","        # and the output is the corresponding word embeddings.\n","        # input_size: num_embeddings (python:int) – size of the dictionary of embeddings\n","        # hidden_size: embedding_dim (python:int) – the size of each embedding vector\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        \n","        # Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n","        # hidden_size: input_size – The number of expected features in the input x\n","        # hidden_size: hidden_size – The number of features in the hidden state h\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        # Create word embedding table based on the data -> tensor table\n","        # view(*shape): Returns a new tensor with the same data as the self tensor but of a different shape.\n","        embedded = self.embedding(input).view(1, 1, -1)\n","\n","        output = embedded\n","\n","        # Execute GRU process\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        # Initiate the first hidden unit\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-u2Rh0T-Anyj","colab_type":"text"},"source":["===================================**Decoder**===================================\n","\n","Mô hình decoder cơ bản có thể được minh họa như sau:\n","\n","![Decoder example](https://drive.google.com/uc?id=15k-FuqT8oCaP3MkHV7W-5OQg072OzQsz)\n","\n","(Source: https://medium.com/@t.schnake/a-formalization-of-a-simple-sequential-encoder-decoder-b31be7e92988)\n","\n","![Pytorch base decoder flow](https://drive.google.com/uc?id=16Mt0xS53HsAczMiwZYbcBRC0x95wA_og)\n","\n","(Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","\n","Class này có các thuộc tính cơ bản sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong tập kết quả của encoder.\n","*   **embedding**: lưu trữ kết quả của bộ vector kết quả từ encoder sau khi thực hiện word embedding (tham khảo thêm về word embedding tại https://machinelearningmastery.com/what-are-word-embeddings/).\n","*   **gru**: cấu trúc GRU sẽ sử dụng.\n","*   **out**: đối tượng linear transformation.\n","*   **softmax**: đối tượng softmax.\n","\n","Trong hàm khởi tạo ta sẽ truyền vào các biến sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong tập kết quả của encoder.\n","*   **output_size**: kích thước của kết quả đầu ra."]},{"cell_type":"code","metadata":{"id":"MCDzHwOAoxy_","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        # A simple lookup table that stores embeddings of a fixed dictionary \n","        # and size. \n","        # This module is often used to store word embeddings and retrieve them using indices. \n","        # The input to the module is a list of indices, \n","        # and the output is the corresponding word embeddings.\n","        # output_size: num_embeddings (python:int) – size of the dictionary of embeddings\n","        # hidden_size: embedding_dim (python:int) – the size of each embedding vector\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        # Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n","        # hidden_size: input_size – The number of expected features in the input x\n","        # hidden_size: hidden_size – The number of features in the hidden state h\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # output_size: out_features – size of each output sample\n","        self.out = nn.Linear(hidden_size, output_size)\n","        \n","        # Applies the log(Softmax(x) function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:\n","        # logSoftmax(xi) = log(exp(xi) / sum_j(exp(xj)))\n","        # dim=1: Input: (*) where * means, any number of additional dimensions\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        # Create word embedding table based on the data -> tensor table\n","        # view(*shape): Returns a new tensor with the same data as the self tensor but of a different shape.\n","        output = self.embedding(input).view(1, 1, -1)\n","        \n","        # Assuring the encoder output is not negative\n","        # Because ReLU output range is (0, +inf)\n","        output = F.relu(output)\n","\n","        # Execute GRU process\n","        output, hidden = self.gru(output, hidden)\n","\n","        # Calculate the softmax\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        # Initiate the first hidden unit\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgQowu4v07Kw","colab_type":"text"},"source":["**Decoder sử dụng attention**\n","\n","Attention là một cơ chế kỹ thuật nhằm giúp giải quyết các giới hạn của kiến trúc encoder-decoder trên các chuỗi dài (chuỗi dữ liệu đầu vào). Attention sẽ giúp tăng tốc độ học tập và cải thiện khả năng dự đoán của mô hình.\n","\n","Do vector ngữ cảnh này có độ dài cố định và là kết quả sau quá trình mã hóa từ dữ liệu đầu vào có độ dài thay đổi, nên vector này sẽ chứa rất nhiều thông tin. Việc mã hóa (decode) vector này sẽ là một áp lực lớn đối với decoder.\n","\n","Cơ chế attention cho phép tại mỗi bước input của decoder, decoder có thể tập trung vào từng phần khác nhau của encoder output. Đầu tiên, chúng ta sẽ tính tập trọng số attention (attention weight). Sau đó ta sẽ lấy tập trọng số này nhân với các vectơ đầu ra của encoder để tạo ra một tổ hợp có trọng số. Kết quả (được gọi là attn_applied) này giúp nhận biết được từng phần của chuỗi đầu vào, và từ đó sẽ giúp decoder chọn được từ (word) đầu ra đúng.\n","\n","Thay vì mã hóa chuỗi đầu vào thành một vectơ ngữ cảnh cố định duy nhất, mô hình chú ý phát triển một vectơ bối cảnh được lọc riêng cho từng bước thời gian t ở đầu ra.\n","\n","![Attention](https://drive.google.com/uc?id=1uK8t6wEnWEfpC4osuankfgH7WMXrx4ec)\n","\n","(Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n"]},{"cell_type":"code","metadata":{"id":"JsiUbVbB8Z04","colab_type":"code","outputId":"10ef3e92-829f-409a-cfc4-7215b23b8f7b","executionInfo":{"status":"ok","timestamp":1587023266142,"user_tz":-420,"elapsed":759,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}},"colab":{"base_uri":"https://localhost:8080/","height":475}},"source":["print(\"Source: https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\")\n","from IPython.display import Image\n","Image(url='https://miro.medium.com/max/576/1*wBHsGZ-BdmTKS7b-BtkqFQ.gif')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Source: https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/576/1*wBHsGZ-BdmTKS7b-BtkqFQ.gif\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"XnZ-mFec9ceH","colab_type":"text"},"source":["Ta sẽ đi xây dựng class decoder có sử dụng attention.\n","\n","![alt text](https://drive.google.com/uc?id=1MZ4vaGKFX5Q8cd-Z40i-rYRHg2yh5pDd)\n","\n","(Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","\n","Class này có các thuộc tính cơ bản sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong tập kết quả của encoder.\n","*   **output_size**: kích thước của đầu ra.\n","*   **dropout_p**: Trong quá trình đào tạo, gán ngẫu nhiên giá trị 0 cho một vài phần tử của decoder input với xác suất p theo phân phối Bernoulli (tiếng việt: https://maths.uel.edu.vn/Resources/Docs/SubDomain/maths/TaiLieuHocTap/ToanUngDung/phn_phi_bernoulli.html, tiếng anh: https://en.wikipedia.org/wiki/Bernoulli_distribution). Mỗi channel sẽ được 0 hóa độc lập trong mỗi lần forward (lan truyền tiến). Đây đã được chứng minh là một kỹ thuật hiệu quả để chuẩn hóa (regularization) và ngăn chặn sự đồng thích ứng (co-adaptation) của các nơ-ron thần kinh như được mô tả trong bài viết https://arxiv.org/pdf/1207.0580.pdf. Hơn nữa, các đầu ra được chia tỷ lệ theo hệ số 1 / (1-p) trong quá trình đào tạo. Điều này có nghĩa là trong quá trình đánh giá, mô-đun chỉ cần tính toán một hàm identity.\n","*   **max_length**: số lượng từ giới hạn cho 1 câu dịch.\n","*   **embedding**: lưu trữ kết quả của bộ vector kết quả từ encoder sau khi thực hiện word embedding (tham khảo thêm về word embedding tại https://machinelearningmastery.com/what-are-word-embeddings/).\n","*   **attn**: .\n","*   **attn_combine**: .\n","*   **dropout**: .\n","*   **gru**: cấu trúc GRU sẽ sử dụng.\n","*   **out**: đối tượng linear transformation.\n","\n","Trong hàm khởi tạo ta sẽ truyền vào các biến sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong tập kết quả của encoder.\n","*   **output_size**: kích thước của kết quả đầu ra.\n","*   **dropout_p**: .\n","*   **max_length**: ."]},{"cell_type":"code","metadata":{"id":"Qj713Xkz9iVO","colab_type":"code","colab":{}},"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        # A simple lookup table that stores embeddings of a fixed dictionary \n","        # and size. \n","        # This module is often used to store word embeddings and retrieve them using indices. \n","        # The input to the module is a list of indices, \n","        # and the output is the corresponding word embeddings.\n","        # output_size: num_embeddings (python:int) – size of the dictionary of embeddings\n","        # hidden_size: embedding_dim (python:int) – the size of each embedding vector\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","\n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size * 2: in_features – size of each input sample\n","        # max_length: out_features – size of each output sample\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","\n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size * 2: in_features – size of each input sample\n","        # hidden_size: out_features – size of each output sample\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","\n","        # During training, randomly zeroes some of the elements of \n","        # the input tensor with probability p \n","        # using samples from a Bernoulli distribution. \n","        # Each channel will be zeroed out independently on every forward call.\n","        # This has proven to be an effective technique for regularization \n","        # and preventing the co-adaptation of neurons \n","        # as described in the paper Improving neural networks by preventing co-adaptation of feature detectors .\n","        # Furthermore, the outputs are scaled by a factor of 1/(1-p) \n","        # during training. This means that during evaluation the module simply computes an identity function.\n","        # dropout_p: p – probability of an element to be zeroed. Default (when don't have parameter): 0.5\n","        # In this constructor, the default parameter (probability) is 0.1\n","        self.dropout = nn.Dropout(self.dropout_p)\n","\n","        # Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n","        # hidden_size: input_size – The number of expected features in the input x\n","        # hidden_size: hidden_size – The number of features in the hidden state h\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        \n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # output_size: out_features – size of each output sample\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        # Create word embedding table based on the data -> tensor table\n","        # view(*shape): Returns a new tensor with the same data as the self tensor but of a different shape.\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        \n","        # Execute dropout process\n","        embedded = self.dropout(embedded)\n","\n","        # Execute attention process\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        # Performs a batch matrix-matrix product\n","        # unsqueeze: Returns a new tensor with a dimension of size one inserted at the specified position.\n","        #            The returned tensor shares the same underlying data with this tensor.\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        # Assuring the encoder output is not negative\n","        # Because ReLU output range is (0, +inf)\n","        output = F.relu(output)\n","\n","        # Execute GRU process\n","        output, hidden = self.gru(output, hidden)\n","\n","        # Calculate the softmax\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        # Initiate the first hidden unit\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-LCqTaEm14K","colab_type":"text"},"source":["**5) Quá trình huấn luyện**\n","\n","Để huấn luyện, đối với mỗi cặp, chúng ta sẽ cần một tensor (trong toán là vector) đầu vào (index của các từ trong câu) và tensor đích (index của các từ trong câu đích). Trong khi tạo các vector này, ta sẽ nối thêm mã thông báo EOS vào cả hai chuỗi."]},{"cell_type":"code","metadata":{"id":"zuPG3lQVngfR","colab_type":"code","colab":{}},"source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)"],"execution_count":0,"outputs":[]}]}