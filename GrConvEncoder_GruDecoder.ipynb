{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GrConvEncoder_GruDecoder.ipynb","provenance":[],"authorship_tag":"ABX9TyNNGSV5qYDDtKJ18FfkpPU7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M6t84m8dOOtu","colab_type":"text"},"source":["**0) Lý thuyết và nguồn dữ liệu**\n","\n","Nguồn dữ liệu được lấy từ trang web: https://github.com/stefan-it/nmt-en-vi/tree/master/data\n","\n","Lý thuyết tham khảo ở các tài liệu:\n","\n","[1].   Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\n","\n","[2].   Rico Sennrich, Barry Haddow (2016). Linguistic Input Features Improve Neural Machine Translation."]},{"cell_type":"markdown","metadata":{"id":"IFCvtgLbOuMr","colab_type":"text"},"source":["**1) Chuẩn bị thư viện và dữ liệu**\n","\n","Đầu tiên ta sẽ chuẩn bị một số thư viện"]},{"cell_type":"code","metadata":{"id":"UaOsHpO0OEcC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"b92dc7ca-52db-4fd0-ed41-9aa44876f635","executionInfo":{"status":"ok","timestamp":1588984046037,"user_tz":-420,"elapsed":7512,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","!pip install unidecode\n","import unidecode\n","import string\n","import re\n","import random\n","import numpy as np\n","import html\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 3.5MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qg0FyOGwO18d","colab_type":"text"},"source":["Chuẩn bị dữ liệu cho chương trình"]},{"cell_type":"code","metadata":{"id":"BkvjuyT0O6XY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"1cb0d6a1-fd65-41ce-dace-1d37f39c614a","executionInfo":{"status":"ok","timestamp":1588984054850,"user_tz":-420,"elapsed":6987,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["!git clone https://github.com/nguyenduyhanlam/PTXS-GTNN\n","!ls\n","import os\n","os.getcwd()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'PTXS-GTNN'...\n","remote: Enumerating objects: 60, done.\u001b[K\n","remote: Counting objects: 100% (60/60), done.\u001b[K\n","remote: Compressing objects: 100% (48/48), done.\u001b[K\n","remote: Total 60 (delta 31), reused 41 (delta 12), pack-reused 0\u001b[K\n","Unpacking objects: 100% (60/60), done.\n","PTXS-GTNN  sample_data\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"msIaA-WOO8-x","colab_type":"text"},"source":["Ta cần chuẩn bị thêm thư viện phục vụ cho việc lemmas hóa."]},{"cell_type":"code","metadata":{"id":"iqAN5p5rO_Ws","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e9fa0f41-3983-4cf9-f7d6-b8844c10e2e2","executionInfo":{"status":"ok","timestamp":1588984065385,"user_tz":-420,"elapsed":9277,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2rgXrhP0PBSj","colab_type":"text"},"source":["**2) Load dữ liệu** <br>\n","Ta sẽ tạo class ngôn ngữ dùng để load dữ liệu từ file text. <br>\n","Class ngôn ngữ gồm có các **thuộc tính** cơ bản sau:\n","*   **name**: tên của ngôn ngữ (English, tiếng Việt,...)\n","*   **word2index**: bộ từ điển.\n","*   **word2count**: bộ đếm từ.\n","*   **index2word**: bộ lưu vị trí các token.\n","*   **n_words**: đếm tổng các lượng từ.\n","\n","Class ngôn ngữ gồm có các **hàm** cơ bản sau:\n","*   **addSentence**: Có tham số là sentence, dùng để truyền vào 1 câu văn bản. Từ câu văn bản này, ta sẽ tách thành các từ (word). Cuối cùng ta sẽ lưu các từ này vào bộ từ điển bằng hàm addWord.\n","*   **addWord**: Có tham số là word, dùng để truyền vào 1 từ (word), nếu từ này chưa có trong bộ từ điển thì sẽ được thêm (add) mới vào bộ từ điển. Nếu đã có rồi ta sẽ tăng số lần xuất hiện (số đếm) của từ này thêm 1 đơn vị."]},{"cell_type":"code","metadata":{"id":"v9n12pc2PF7S","colab_type":"code","colab":{}},"source":["SOS_token = 0 #Start Of Sequence\n","EOS_token = 1 #End Of Sequence\n","PAD_token = 2 # Used for padding short sentences\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7Bk1uH_PO0p","colab_type":"text"},"source":["Ta cần triển khai thêm một số hàm bổ trợ như:\n","*   **remove_accent**: dùng để loại bỏ các dấu thanh (sắc, hỏi, huyền, ngã, nặng,...).\n","*   **normalizeString**: Dùng để chuẩn hóa câu để chạy chương trình. Đầu tiên ta sẽ loại bỏ hết các dấu thanh ra khỏi câu. Sau đó ta sẽ tách các dấu câu (dấu phẩy, dấu chấm, dấu chấm hỏi, dấu chấm thang,...) ra khỏi từ. Ngoài ra ta sẽ chuyển bớt một số dạng thể ngắn của tiếng anh về dạng thường.\n","\n","Ví dụ ta có câu: \"Ăn quả, nhớ kẻ trồng cây.\". Sau khi chạy hàm **normalizeString** ta sẽ được kết quả là: \"An qua , nho ke trong cay .\""]},{"cell_type":"code","metadata":{"id":"sWCIsp5BPUS_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"88cf88b6-dc8a-453a-8182-0a3393e20ad7","executionInfo":{"status":"ok","timestamp":1588984072556,"user_tz":-420,"elapsed":750,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["# remove all the accents\n","def remove_accent(utf8_str):\n","    return unidecode.unidecode(utf8_str)\n","\n","contractions = {\n","\"ain 't\": \"am not / are not\",\n","\"aren 't\": \"are not / am not\",\n","\"can 't\": \"cannot\",\n","\"can 't 've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could 've\": \"could have\",\n","\"couldn 't\": \"could not\",\n","\"couldn 't 've\": \"could not have\",\n","\"didn 't\": \"did not\",\n","\"doesn 't\": \"does not\",\n","\"don 't\": \"do not\",\n","\"hadn 't\": \"had not\",\n","\"hadn 't 've\": \"had not have\",\n","\"hasn 't\": \"has not\",\n","\"haven 't\": \"have not\",\n","\"he 'd\": \"he had / he would\",\n","\"he 'd 've\": \"he would have\",\n","\"he 'll\": \"he shall / he will\",\n","\"he 'll 've\": \"he shall have / he will have\",\n","\"he 's\": \"he has / he is\",\n","\"how'd\": \"how did\",\n","\"how'd 'y\": \"how do you\",\n","\"how 'll\": \"how will\",\n","\"how 's\": \"how has / how is\",\n","\"i 'd\": \"I had / I would\",\n","\"i 'd 've\": \"I would have\",\n","\"i 'll\": \"I shall / I will\",\n","\"i 'll 've\": \"I shall have / I will have\",\n","\"i 'm\": \"I am\",\n","\"i 've\": \"I have\",\n","\"isn 't\": \"is not\",\n","\"it 'd\": \"it had / it would\",\n","\"it 'd've\": \"it would have\",\n","\"it 'll\": \"it shall / it will\",\n","\"it 'll've\": \"it shall have / it will have\",\n","\"it 's\": \"it has / it is\",\n","\"let 's\": \"let us\",\n","\"ma 'am\": \"madam\",\n","\"mayn 't\": \"may not\",\n","\"might 've\": \"might have\",\n","\"mightn 't\": \"might not\",\n","\"mightn 't 've\": \"might not have\",\n","\"must 've\": \"must have\",\n","\"mustn 't\": \"must not\",\n","\"mustn 't 've\": \"must not have\",\n","\"needn 't\": \"need not\",\n","\"needn 't 've\": \"need not have\",\n","\"o 'clock\": \"of the clock\",\n","\"oughtn 't\": \"ought not\",\n","\"oughtn 't 've\": \"ought not have\",\n","\"shan 't\": \"shall not\",\n","\"sha 'n 't\": \"shall not\",\n","\"shan 't 've\": \"shall not have\",\n","\"she 'd\": \"she had / she would\",\n","\"she 'd've\": \"she would have\",\n","\"she 'll\": \"she shall / she will\",\n","\"she 'll've\": \"she shall have / she will have\",\n","\"she 's\": \"she has / she is\",\n","\"should 've\": \"should have\",\n","\"shouldn 't\": \"should not\",\n","\"shouldn 't 've\": \"should not have\",\n","\"so 've\": \"so have\",\n","\"so 's\": \"so as / so is\",\n","\"that 'd\": \"that would / that had\",\n","\"that 'd 've\": \"that would have\",\n","\"that 's\": \"that has / that is\",\n","\"there 'd\": \"there had / there would\",\n","\"there 'd 've\": \"there would have\",\n","\"there 's\": \"there has / there is\",\n","\"they 'd\": \"they had / they would\",\n","\"they 'd 've\": \"they would have\",\n","\"they 'll\": \"they shall / they will\",\n","\"they 'll 've\": \"they shall have / they will have\",\n","\"they 're\": \"they are\",\n","\"they 've\": \"they have\",\n","\"to 've\": \"to have\",\n","\"wasn 't\": \"was not\",\n","\"we 'd\": \"we had / we would\",\n","\"we 'd 've\": \"we would have\",\n","\"we 'll\": \"we will\",\n","\"we 'll 've\": \"we will have\",\n","\"we 're\": \"we are\",\n","\"we 've\": \"we have\",\n","\"weren 't\": \"were not\",\n","\"what 'll\": \"what shall / what will\",\n","\"what 'll 've\": \"what shall have / what will have\",\n","\"what 're\": \"what are\",\n","\"what 's\": \"what has / what is\",\n","\"what 've\": \"what have\",\n","\"when 's\": \"when has / when is\",\n","\"when 've\": \"when have\",\n","\"where 'd\": \"where did\",\n","\"where 's\": \"where has / where is\",\n","\"where 've\": \"where have\",\n","\"who 'll\": \"who shall / who will\",\n","\"who 'll 've\": \"who shall have / who will have\",\n","\"who 's\": \"who has / who is\",\n","\"who 've\": \"who have\",\n","\"why 's\": \"why has / why is\",\n","\"why 've\": \"why have\",\n","\"will 've\": \"will have\",\n","\"won 't\": \"will not\",\n","\"won 't 've\": \"will not have\",\n","\"would 've\": \"would have\",\n","\"wouldn 't\": \"would not\",\n","\"wouldn 't 've\": \"would not have\",\n","\"y 'all\": \"you all\",\n","\"y 'all 'd\": \"you all would\",\n","\"y 'all 'd 've\": \"you all would have\",\n","\"y 'all 're\": \"you all are\",\n","\"y 'all 've\": \"you all have\",\n","\"you 'd\": \"you had / you would\",\n","\"you 'd 've\": \"you would have\",\n","\"you 'll\": \"you shall / you will\",\n","\"you 'll 've\": \"you shall have / you will have\",\n","\"you 're\": \"you are\",\n","\"you 've\": \"you have\"\n","}\n","\n","# Normalize the string (marks and words are seperated, words don't contain accents,...)\n","def normalizeString(s):\n","    # Remove all the accents first.\n","    s = remove_accent(html.unescape(s))\n","    # Seperate words and marks by adding spaces between them\n","    marks = '[.!?,-${}()]'\n","    r = \"([\"+\"\\\\\".join(marks)+\"])\"\n","    s = re.sub(r, r\" \\1 \", s)\n","    # replace continuous spaces with a single space\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    \n","    # Convert to writing form\n","    for c in contractions:\n","        if c in s:\n","            s = s.replace(c, contractions[c].lower())\n","\n","    return s\n","\n","# Example\n","ex_s = \"Ăn quả, nhớ kẻ trồng cây.\"\n","print(normalizeString(ex_s)) # result will be \"An qua , nho ke trong cay .\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["An qua , nho ke trong cay .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MvheSe5qPXER","colab_type":"text"},"source":["Cuối cùng ta sẽ thực hiện việc load dữ liệu."]},{"cell_type":"code","metadata":{"id":"nL2VxZCvPYK5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"013c0d1a-47a6-43cc-b3ff-afeb324a96bf","executionInfo":{"status":"ok","timestamp":1588984091727,"user_tz":-420,"elapsed":15160,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["def readLangs(lang1, lang2):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines_language1 = open('PTXS-GTNN/train.%s' % lang1, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","    lines_language2 = open('PTXS-GTNN/train.%s' % lang2, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","\n","    # Normalize all the lines\n","    data_language1 = [normalizeString(l.lower().strip()) for l in lines_language1]\n","    data_language2 = [normalizeString(l.lower().strip()) for l in lines_language2]\n","\n","    # Prepare return values\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","    data = list(zip(data_language1, data_language2))\n","\n","    return input_lang, output_lang, data\n","\n","# Test the function\n","lang1 = \"en\"\n","lang2 = \"vi\"\n","input_lang, output_lang, data = readLangs(lang1, lang2)\n","print(\"Language 1:\", input_lang.name)\n","print(\"Language 2:\", output_lang.name)\n","print(random.choice(data))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Reading lines...\n","Language 1: en\n","Language 2: vi\n","('and in case you do not think that this really has meaning for you , remember that cloning is possible , and that involves going through childhood again , in which case you shall / you will want to be heard just like my generation .', 'va , neu cac ban khong nghi dieu nay co y nghia hay nho rang khoa hoc vo tinh da thanh hien thuc , va co the ta se quay lai thoi tho au , trong truong hop do , ban se muon duoc lang nghe giong nhu the he cua toi .')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ksA1rJRcPa-8","colab_type":"text"},"source":["**3) Chuẩn bị dữ liệu**\n","\n","Chương trình sẽ chạy nhanh hơn khi chúng ta lọc bộ dữ liệu thành các câu ngắn và đơn giản. Ở đây ta sẽ giới hạn cho 1 câu có tối đa là 10 từ (bao gồm cả dấu chấm câu). Đồng thời ta viết hàm thực hiện lemmatization."]},{"cell_type":"code","metadata":{"id":"ZWhqhIYmPh3i","colab_type":"code","colab":{}},"source":["MAX_LENGTH = 10\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","def lemmatizer(sentence):\n","    for word in sentence.split():\n","        sentence = sentence.replace(word, wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n","    return sentence"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w88pdIt7PkWG","colab_type":"text"},"source":["Sau quá trình biến đổi chúng ta sẽ có được tập dữ liệu để chuẩn bị cho huấn luyện."]},{"cell_type":"code","metadata":{"id":"0pAK6zG1PlpC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"538b9612-117d-478e-ba83-e468d2e584e4","executionInfo":{"status":"ok","timestamp":1588984116805,"user_tz":-420,"elapsed":17486,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["def prepareData(lang1, lang2):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        # Lemmatization process\n","        inWord = lemmatizer(pair[0])\n","        outWord = lemmatizer(pair[1])\n","        # Add word to dictionaries after lemmatization\n","        input_lang.addSentence(inWord)\n","        output_lang.addSentence(outWord)\n","    print(\"Counted words:\")\n","    print(\"Language 1:\", input_lang.name, \"There are\", input_lang.n_words, \"different words\")\n","    print(\"Language 2:\", output_lang.name, \"There are\", output_lang.n_words, \"different words\")\n","    return input_lang, output_lang, pairs\n","\n","\n","input_lang, output_lang, pairs = prepareData('en', 'vi')\n","print(random.choice(pairs))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Reading lines...\n","Read 133317 sentence pairs\n","Trimmed to 14655 sentence pairs\n","Counting words...\n","Counted words:\n","Language 1: en There are 7111 different words\n","Language 2: vi There are 3114 different words\n","('miniaturization as well .', 'cong nghe thu nho cung vay .')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Iwu9e9LPoH4","colab_type":"text"},"source":["**4) Mô hình Encoder-Decoder**\n","\n","Đầu tiên ta sẽ đi xây dưng bộ encoder trước.\n","\n","---\n","\n","===================================**Encoder**===================================\n","\n","Encoder ta sẽ sử dụng có tên Gated Recursive Convolutional Neural Network (GrConv).\n","\n","Mô hình encoder như sau:\n","\n","![GrConvEncoder](https://drive.google.com/uc?id=19gFbQrva-3ZvvBo_u_Bv8PAbIf3a_Njh)\n","\n","                             Minh họa cấu trúc của một bộ GrConv encoder [1]\n","\n","Các hidden unit của mô hình sẽ được tính toán theo quy trình sau:\n","\n","![GrConv hidden unit](https://drive.google.com/uc?id=1dzjhmNiaLRLqrumo8WCAmZkFo9xcd8aj)\n","\n","                             Quy trình tính toán cho 1 hidden unit\n","\n","Ta sẽ đi xây dựng một class có tên là GrConv cho bộ GrConv encoder.\n","\n","Class này có các thuộc tính cơ bản sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong bộ từ điển.\n","*   **embedding**: lưu trữ kết quả của bộ từ điển sau khi thực hiện word embedding (tham khảo thêm về word embedding tại https://machinelearningmastery.com/what-are-word-embeddings/).\n","*   **WL**: trọng số của node trái.\n","*   **WR**: trọng số của node phải.\n","*   **GL**: trọng số của cổng trái.\n","*   **GR**: trọng số của cổng phải.\n","*   **sigm**: khởi tạo quá trình tính toán sigmoid.\n","\n","Trong hàm khởi tạo ta sẽ truyền vào các biến sau:\n","*   **input_size**: kích thước của bộ từ điển (số lượng từ có trong bộ từ điển).\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong bộ từ điển."]},{"cell_type":"code","metadata":{"id":"bQUyKAfwVXST","colab_type":"code","colab":{}},"source":["class GrConv(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(GrConv, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        # A simple lookup table that stores embeddings of a fixed dictionary \n","        # and size. \n","        # This module is often used to store word embeddings and retrieve them using indices. \n","        # The input to the module is a list of indices, \n","        # and the output is the corresponding word embeddings.\n","        # input_size: num_embeddings (python:int) – size of the dictionary of embeddings\n","        # hidden_size: embedding_dim (python:int) – the size of each embedding vector\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        \n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # hidden_size: out_features – size of each output sample\n","        # Represent: WL in (WL * h_(t-1, j-1))\n","        self.WL = nn.Linear(self.hidden_size, self.hidden_size, bias=None)\n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # hidden_size: out_features – size of each output sample\n","        # Represent: WR in (WR * h_(t-1, j))\n","        self.WR = nn.Linear(self.hidden_size, self.hidden_size, bias=None) \n","        \n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # hidden_size: out_features – size of each output sample\n","        # Represent: GL in (GL * h_(t-1, j-1))\n","        self.GL = nn.Linear(self.hidden_size, 3*self.hidden_size, bias=None)\n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # hidden_size: out_features – size of each output sample\n","        # Represent: GR in (GR * h_(t-1, j-1))\n","        self.GR = nn.Linear(self.hidden_size, 3*self.hidden_size, bias=None)\n","        \n","        self.sigm = torch.nn.Sigmoid()\n","\n","    def forward(self, input):\n","        # Create the array to store all the next hidden units\n","        next_hiddens = []\n","        \n","        # Forwarding process\n","        for ei in range(len(input) - 1):\n","            left_node = input[ei]\n","            right_node = input[ei + 1]\n","            \n","            # Calculate h~\n","            center_node = self.sigm(self.WL(left_node) + self.WR(right_node))\n","            \n","            # Calculate (GL*left_node + GR*right_node)\n","            gate_feature = self.GL(left_node) + self.GR(right_node)\n","            \n","            # Calculate exp(GL*left_node + GR*right_node)\n","            gate_feature = torch.exp(gate_feature)\n","            \n","            # Get gate batches [0:hidden_unit],[hidden_unit:2*hidden_unit],[2*hidden_unit:3*hidden_unit]\n","            omega_scores = tuple(gate_feature[:, :, i*self.hidden_size : (i+1)*self.hidden_size].unsqueeze(-1) for i in range(3))\n","            omega_scores = torch.cat(omega_scores, 3)\n","            \n","            # Calculate Z = sum(exp(GL*left_node + GR*right_node))_k for k in [1,3]\n","            Z = torch.sum(omega_scores, 3)\n","            \n","            # Calculate (1/Z)*(exp(GL*left_node + GR*right_node))\n","            divide_Z = 1/Z.unsqueeze(-1).expand(*Z.size(), 3)\n","            omega_norm = divide_Z * omega_scores\n","            \n","            # Calculate the next hidden_unit=omega_left*left_node + omega_right*right_node + omega_center*center_node\n","            next_node = torch.mul(left_node,omega_norm[:,:,:,0]) + torch.mul(right_node,omega_norm[:,:,:,1]) + torch.mul(center_node,omega_norm[:,:,:,2])\n","            \n","            # Save the result to array\n","            next_hiddens.append(next_node)\n","            \n","        return next_hiddens\n","\n","    def initHidden(self, input):\n","        # Save input tensor size\n","        input_length = input.size(0)\n","    \n","        # Embedding storage\n","        embedding_storage = []\n","        \n","        # Embedding all the inputs and save it to storage\n","        for ei in range(input_length):\n","            embedded = self.embedding(input[ei]).view(1, 1, -1)\n","            embedding_storage.append(embedded)\n","            \n","        return embedding_storage"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjiUEUMnVnXG","colab_type":"text"},"source":["===================================**Decoder**===================================\n","\n","Mô hình decoder cơ bản có thể được minh họa như sau:\n","\n","![Decoder example](https://drive.google.com/uc?id=15k-FuqT8oCaP3MkHV7W-5OQg072OzQsz)\n","\n","(Source: https://medium.com/@t.schnake/a-formalization-of-a-simple-sequential-encoder-decoder-b31be7e92988)\n","\n","![Pytorch base decoder flow](https://drive.google.com/uc?id=16Mt0xS53HsAczMiwZYbcBRC0x95wA_og)\n","\n","(Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","\n","Class này có các thuộc tính cơ bản sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong tập kết quả của encoder.\n","*   **embedding**: lưu trữ kết quả của bộ vector kết quả từ encoder sau khi thực hiện word embedding (tham khảo thêm về word embedding tại https://machinelearningmastery.com/what-are-word-embeddings/).\n","*   **gru**: cấu trúc GRU sẽ sử dụng.\n","*   **out**: đối tượng linear transformation.\n","*   **softmax**: đối tượng softmax.\n","\n","Trong hàm khởi tạo ta sẽ truyền vào các biến sau:\n","*   **hidden_size**: kích thước (số chiều) của vector của mỗi phần tử trong tập kết quả của encoder.\n","*   **output_size**: kích thước của kết quả đầu ra."]},{"cell_type":"code","metadata":{"id":"zEnI3hkpVtAI","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        # A simple lookup table that stores embeddings of a fixed dictionary \n","        # and size. \n","        # This module is often used to store word embeddings and retrieve them using indices. \n","        # The input to the module is a list of indices, \n","        # and the output is the corresponding word embeddings.\n","        # output_size: num_embeddings (python:int) – size of the dictionary of embeddings\n","        # hidden_size: embedding_dim (python:int) – the size of each embedding vector\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        # Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n","        # hidden_size: input_size – The number of expected features in the input x\n","        # hidden_size: hidden_size – The number of features in the hidden state h\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","        # Applies a linear transformation to the incoming data: y = xA^T + b\n","        # hidden_size: in_features – size of each input sample\n","        # output_size: out_features – size of each output sample\n","        self.out = nn.Linear(hidden_size, output_size)\n","        \n","        # Applies the log(Softmax(x) function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:\n","        # logSoftmax(xi) = log(exp(xi) / sum_j(exp(xj)))\n","        # dim=1: Input: (*) where * means, any number of additional dimensions\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        # Create word embedding table based on the data -> tensor table\n","        # view(*shape): Returns a new tensor with the same data as the self tensor but of a different shape.\n","        output = self.embedding(input).view(1, 1, -1)\n","        \n","        # Assuring the encoder output is not negative\n","        # Because ReLU output range is (0, +inf)\n","        output = F.relu(output)\n","\n","        # Execute GRU process\n","        output, hidden = self.gru(output, hidden)\n","\n","        # Calculate the softmax\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        # Initiate the first hidden unit\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CcMM_KcVxt8","colab_type":"text"},"source":["**5) Chuẩn bị quá trình huấn luyện**\n","\n","Để huấn luyện, đối với mỗi cặp, chúng ta sẽ cần một tensor (vector) đầu vào (index của các từ trong câu) và tensor đích (index của các từ trong câu đích). Trong khi tạo các vector này, ta sẽ nối thêm mã thông báo EOS vào cả hai chuỗi."]},{"cell_type":"code","metadata":{"id":"m4NNj4i2V0ki","colab_type":"code","colab":{}},"source":["def indexesFromSentence(lang, sentence):\n","    # Get the counter for each word in the sentence\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    # Get the counter\n","    indexes = indexesFromSentence(lang, sentence)\n","    # Append token to the end of the array\n","    indexes.append(EOS_token)\n","    # Return the vector\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    # Create input vector\n","    input_tensor = tensorFromSentence(input_lang, lemmatizer(pair[0]))\n","    # Create output vector\n","    target_tensor = tensorFromSentence(output_lang, lemmatizer(pair[1]))\n","    # Return result\n","    return (input_tensor, target_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6ZY7ZRRV6Ya","colab_type":"text"},"source":["**6) Quá trình huấn luyện**\n","\n","Để đào tạo, ta sẽ đưa dữ liệu đầu vào đi qua encoder, ta sẽ tiến hành theo dõi các kết quả đầu ra cũng như các giá trị trạng thái ẩn của encoder. Sau đó, ta sẽ cung cấp cho decoder token thông báo <SOS> làm tín hiệu thông báo cho decoder khởi tạo đầu vào đầu tiên, đồng thời trạng thái ẩn cuối cùng của encoder sẽ làm trạng thái ẩn đầu tiên cho decoder.\n","\n","\"Teacher forcing\" là một khái niệm ám chỉ về việc ta sẽ sử dụng các giá trị đầu ra mong muốn cho mỗi đầu vào tiếp theo, thay vì ta sử dụng kết quả mà decoder dự đoán được cho các đầu vào tiếp theo. Việc làm này sẽ khiến cho chương trình hội tụ nhanh hơn, trong một số trường hợp việc làm dụng điều này sẽ khiến cho chương trình không ổn định (tham khảo thêm: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n","\n","Theo trực giác, ta có thể quan sát kết quả đầu ra của các mạng \"teacher-forcing\" có cấu trúc ngữ pháp ổn định tuy nhiên kết quả dịch lại không được chính xác. Điều này cho thấy rằng mạng \"teacher-forcing\" có thể học được cấu trúc ngữ pháp, tuy nhiên việc dịch nghĩa của nó lại không được tốt."]},{"cell_type":"code","metadata":{"id":"ii4mY1TwV8S0","colab_type":"code","colab":{}},"source":["teacher_forcing_ratio = 0.5\n","\n","# Training function\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    \n","    # Intiatie encoder process\n","    encoder_hiddens = encoder.initHidden(input_tensor)\n","\n","    # Clears the gradients of all optimized torch.Tensor.\n","    encoder_optimizer.zero_grad()\n","    # Clears the gradients of all optimized torch.Tensor.\n","    decoder_optimizer.zero_grad()\n","\n","    # Save input tensor size\n","    input_length = input_tensor.size(0)\n","    # Save output tensor size\n","    target_length = target_tensor.size(0)\n","\n","    # Initiate loss value\n","    loss = 0\n","\n","    # for each encoder input\n","    for t in range(input_length - 1):\n","        # Execute encoder process\n","        encoder_hiddens = encoder(encoder_hiddens)\n","\n","    # Initiate decoder input\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    # Assign encoder result to the 1st decoder hidden unit\n","    decoder_hidden = encoder_hiddens[0]\n","\n","    # Randomly using teacher_forcing feature\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            # Execute decoder process\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            # Add loss\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            # Execute decoder process\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            # Add loss\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    # Backpropagation\n","    loss.backward()\n","\n","    # Performs a single optimization step.\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOadlDLbWJw_","colab_type":"text"},"source":["Ta có thể theo dõi lượng thời gian chương trình tiến hành huấn luyện bằng một số hàm sau:"]},{"cell_type":"code","metadata":{"id":"Iri3kq0RWLDH","colab_type":"code","colab":{}},"source":["import time\n","import math\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pRyuLCrZWNac","colab_type":"text"},"source":["Ta viết thêm hàm vẽ biểu đồ biểu diễn sự thay đổi của giá trị loss"]},{"cell_type":"code","metadata":{"id":"LHQxE2G3WP2A","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","%matplotlib inline\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nlYUubJjWSAa","colab_type":"text"},"source":["Toàn bộ quá trình đào tạo trông như thế này:\n","*   Bắt đầu tính thời gian huấn luyện.\n","*   Khởi tạo bộ tối ưu hóa (optimizer) và bộ tiêu chuẩn (criterion).\n","*   Tạo một tập các cặp đào tạo (training pair).\n","*   Khởi tạo mảng loss lưu từng giá trị loss tại các thời điểm nhằm phục vụ cho việc vẽ biểu đồ.\n","\n","Cuối cùng ta sẽ gọi hàm train để thực hiện quá trình đào tạo. Kết thúc 1 lượt đào tạo ta sẽ thông báo một vài giá trị như giá trị mất mát (loss) trung bình hiện tại, thời gian huấn luyện hiện tại,..."]},{"cell_type":"code","metadata":{"id":"a1o90gpUWUzC","colab_type":"code","colab":{}},"source":["def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    # Implements stochastic gradient descent (optionally with momentum).\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","\n","    # Randomly choose some pairs in the dataset for training process\n","    training_pairs = [tensorsFromPair(random.choice(pairs))\n","                      for i in range(n_iters)]\n","    # The negative log likelihood loss.\n","    # The input given through \n","    # a forward call is expected to contain log-probabilities of each class. \n","    # Input has to be a Tensor of size either (minibatch,C)\n","    # or or (minibatch, C, d1, d2, ..., dK) with K≥1 for the K-dimensional case.\n","    # Default: loss = sum((1/sum(w_(y_n))) * loss_n), loss_n = -w*x_(n,y_n)\n","    criterion = nn.NLLLoss()\n","\n","    # for each sample\n","    for iter in range(1, n_iters + 1):\n","        # Get input data and target data\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","        \n","        # Execute training process\n","        loss = train(input_tensor, target_tensor, encoder,\n","                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        # Print info\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    # Draw plot\n","    showPlot(plot_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L0p33Uu9WW7h","colab_type":"text"},"source":["**7) Đánh giá (evaluation)**\n","\n","Quá trình đánh giá chủ yếu cũng gần giống như đào tạo, tuy nhiên nó khác với quá trình đào tạo ở chỗ nó sẽ không đi so sánh giữa kết quả dự đoán với giá trị mong muốn (target data). Quá trình đánh giá đơn giản chỉ là ta sẽ đem các giá trị dự đoán mà decoder đã thực hiện được nạp vào lại chính nó tại mỗi bước. Mỗi lần decoder dự đoán một từ, ta sẽ thêm từ đó vào chuỗi đầu ra, khi decoder dự đoán mã thông báo (token) EOS, ta sẽ tiến hành dừng quá trình thực hiện. Chúng ta cũng nên cần lưu trữ các giá trị đầu ra của decoder attention nhằm phục vụ cho các yêu cầu hiển thị về sau."]},{"cell_type":"code","metadata":{"id":"C_noqE4eWZVU","colab_type":"code","colab":{}},"source":["def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    # Disable the gradient computation.\n","    with torch.no_grad():\n","        # Get input sample\n","        input_tensor = tensorFromSentence(input_lang, lemmatizer(sentence))\n","        input_length = input_tensor.size()[0]\n","\n","        # Intiatie encoder process\n","        encoder_hiddens = encoder.initHidden(input_tensor)\n","\n","        # for each encoder input\n","        for t in range(input_length - 1):\n","            # Execute encoder process\n","            encoder_hiddens = encoder(encoder_hiddens)\n","\n","        # Initiate decoder input\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","\n","        # Assign encoder result to the 1st decoder hidden unit\n","        decoder_hidden = encoder_hiddens[0]\n","\n","        # Initiate decoder output\n","        decoded_words = []\n","\n","        # for each decoder input\n","        for di in range(max_length):\n","            # Execute decoder process\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88kYwvFeYTEK","colab_type":"text"},"source":["Ta có thể thực hiện đánh giá bằng việc chọn ngẫu nhiên một số dữ liệu từ tập dữ liệu:"]},{"cell_type":"code","metadata":{"id":"bN2uZwNIYUyC","colab_type":"code","colab":{}},"source":["def evaluateRandomly(encoder, decoder, n=10):\n","    for i in range(n):\n","    #for pair in pairs:\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words = evaluate(encoder, decoder, pair[0])\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryBVbDXKYfre","colab_type":"text"},"source":["**8) Thực hiện huấn luyện và đánh giá**\n","\n","Đầu tiên ta sẽ cho chương trình thực hiện việc huấn luyện mô hình."]},{"cell_type":"code","metadata":{"id":"6dtWKBTJYs7G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":537},"outputId":"b2cbd770-37d5-44df-b671-bcbd31e9310c","executionInfo":{"status":"ok","timestamp":1588992138081,"user_tz":-420,"elapsed":7973743,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["hidden_size = 250\n","encoder1 = GrConv(input_lang.n_words, hidden_size).to(device)\n","decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","trainIters(encoder1, decoder1, 75000, print_every=5000)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["9m 1s (- 126m 22s) (5000 6%) 4.2312\n","18m 2s (- 117m 14s) (10000 13%) 3.8502\n","26m 54s (- 107m 37s) (15000 20%) 3.6050\n","35m 46s (- 98m 22s) (20000 26%) 3.5049\n","44m 27s (- 88m 54s) (25000 33%) 3.3945\n","53m 4s (- 79m 37s) (30000 40%) 3.2603\n","61m 50s (- 70m 40s) (35000 46%) 3.1740\n","70m 46s (- 61m 55s) (40000 53%) 3.1131\n","79m 42s (- 53m 8s) (45000 60%) 3.0049\n","88m 32s (- 44m 16s) (50000 66%) 2.9307\n","97m 26s (- 35m 26s) (55000 73%) 2.8460\n","106m 13s (- 26m 33s) (60000 80%) 2.7794\n","115m 1s (- 17m 41s) (65000 86%) 2.7385\n","123m 52s (- 8m 50s) (70000 93%) 2.6760\n","132m 52s (- 0m 0s) (75000 100%) 2.6592\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"n6yHq893YkcP","colab_type":"text"},"source":["Sau đó ta sẽ tiến hành thực hiện đánh giá."]},{"cell_type":"code","metadata":{"id":"kuKl_oPPZC3f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"outputId":"7595541d-4881-457e-ff83-2ffffb1d790e","executionInfo":{"status":"ok","timestamp":1588992987561,"user_tz":-420,"elapsed":722,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["evaluateRandomly(encoder1, decoder1)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["> because it has to end .\n","= boi vi tat ca phai ket thuc .\n","< vi vi vi thuc vi can phai . . <EOS>\n","\n","> in lebanon , it was not a book .\n","= o libang , khong phai la quyeng sach .\n","< mot khong khong khong phai la mot mot . .\n","\n","> but is he happy ?\n","= nhung lieu anh ay co vui khong ?\n","< nhung anh ay co be ? <EOS>\n","\n","> it weighs less than a gram .\n","= no nang khong den 1 gram\n","< no la mot thanh nguoi dan <EOS>\n","\n","> that bothers me the most .\n","= do la dieu khien toi phien muon nhat .\n","< hay do , hay do toi . <EOS>\n","\n","> that does not work .\n","= lam the khong duoc .\n","< dieu do khong khong co . <EOS>\n","\n","> the exact converse is also true .\n","= gia tri nguoc chuan xac cung dung\n","< gia cung cung la kien cung . <EOS>\n","\n","> art is our weapon .\n","= nghe thuat la vu khi cua chung toi .\n","< chung toi la su mo cua chung ta . <EOS>\n","\n","> they should be fluffy , not hard . \"\n","= nen mem , khong nen cung qua . \"\n","< ho nen mac , va <EOS>\n","\n","> so this is a rather fraught situation .\n","= day la mot tinh huong day kich tinh .\n","< day la mot mot dieu tu tinh . <EOS>\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2B4uTG9jZWvj","colab_type":"text"},"source":["**9) Chạy trên câu dịch bất kỳ**"]},{"cell_type":"code","metadata":{"id":"Q6g9FTvnZhsA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"9a37d400-4539-4467-8358-8a5843c04999","executionInfo":{"status":"ok","timestamp":1588992995401,"user_tz":-420,"elapsed":724,"user":{"displayName":"Nguyễn Duy Hàn Lâm","photoUrl":"","userId":"15262702392422763405"}}},"source":["def evaluateAndShowAttention(input_sentence):\n","    output_words = evaluate(\n","        encoder1, decoder1, input_sentence)\n","    print('input =', input_sentence)\n","    print('output =', ' '.join(output_words))\n","\n","evaluateAndShowAttention(normalizeString(\"i know all the possibility .\".lower().strip()))\n","\n","evaluateAndShowAttention(normalizeString(\"they kill me .\".lower().strip()))\n","\n","evaluateAndShowAttention(normalizeString(\"let me tell you a story .\".lower().strip()))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["input = i know all the possibility .\n","output = toi biet cac biet day . <EOS>\n","input = they kill me .\n","output = ho giet ho toi . <EOS>\n","input = let me tell you a story .\n","output = de toi toi cho ban mot cau chuyen . <EOS>\n"],"name":"stdout"}]}]}